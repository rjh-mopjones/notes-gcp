**Platform Fundamentals**

**Cloud IAM**

 - Cloud IAM is a sophisticated system built on top of email-like address names, job-type roles, and granular permissions

_ **Cloud Identity and Access** _

 - Identity Access Management is a way of identifying who can do what on which resource.
   - The who can be a person, group, or application. The what refers to specific privileges or actions, and the resource could be any GCP service.
   - For example, I could give you the privilege or role of Compute Viewer. 
     - This provides you with read-only access to get and list Compute Engine resources, without being able to read the data stored on them.
 - Cloud IAM is compoased of different objects
 - Google Cloud Platform resources are organized hierarchically
 - The Organization node is the root node in this hierarchy, 
   - folders are the children of the organization, 
   - projects are the children of the folders, 
   - and the individual resources are the children of projects
   - Each resource has exactly one parent.
 - Cloud IAM allows you to set policies at all of these levels, where a policy contains a set of roles and role members
   - The organization resource represents your company. Cloud IAM roles granted at this level are inherited by all resources under the organization.
   - The folder resource could represent your department. Cloud IAM roles granted at this level are inherited by all resources that the folder contains
   - Projects represent a trust boundary within your company. Services within the same project have a default level of trust. 
 - The Cloud IAM policy hierarchy always follows the same path as the GCP resource hierarchy, 
   - which means that if you change the resource hierarchy, the policy hierarchy also changes
   - For example, moving a project into a different organization will update the project's Cloud IAM policy to inherit from the new organization's Cloud IAM policy
 - Also, child policies cannot restrict access granted at the parent level. 
   - For example, if I grant you the Editor role for Department X, and I grant you the Viewer role at the bookshelf project level, you still have the Editor role for that project
     - Therefore, it is a best practice is to follow the principle of least privilege
     - The principle applies to identities, roles, and resources.
     - Always select the smallest scope that’s necessary for the task in order to reduce your exposure to risk

_ **Organization** _

 - Organization Node is a root node for google cloud resources
 - the organization resource is the root node in the GCP resource hierarchy. This node has many roles, like the Organization Admin
 - The Organization Admin provides a user like Bob with access to administer all resources belonging to his organization, which is useful for auditing
 - There is also a Project Creator role, which allows a user like Alice to create projects within her organization
 - Creating and Managing Organizations
   - When a user with a Workspace or Cloud Identity account creates a Google Cloud Project, an Organization resource is automatically provisioned for them
     - Then, Google Cloud communicates its availability to the Workspace or Cloud Identity super admins. 
     - These super admin accounts should be used carefully because they have a lot of control over your organization and all the resources underneath it.
   - Workspace/Cloud Identity super admin and Google Cloud Organization admin are key roles in the setup process lifecycle control for the Organization resource. 
     - The two roles are generally assigned to different users or groups, although this depends on the organization structure and needs.
   - In the context of Google Cloud Organization setup, the Workspace or Cloud Identity super administrator responsibilities are
     - Assign the Organization admin role to some users
     - Be a point of contact in case of recovery issues.
     - Control the lifecycle of the Workspace or Cloud Identity account and Organization resource.
   - The responsibilities of the Organization admin role are
     - Define IAM policies.
     - Determine the structure of the resource hierarchy
     - Delegate responsibility over critical components such as Networking, Billing, and Resource Hierarchy through IAM roles
     - Following the principle of least privilege, this role does not include the permission to perform other actions, such as creating folders
       - To get these permissions, an Organization admin must assign additional roles to their account. 
 - Folders 
   - Folders provide an additional grouping mechanism and isolation boundary between projects
   - Folders can be used to model different legal entities, departments, and teams within a company. 
     - For example, a first level of folders could be used to represent the main departments in your organization, like departments X and Y
     - Because folders can contain projects and other folders, each folder could then include other sub-folders to represent different teams, like teams A and B.
     - Each team folder could contain additional sub-folders to represent different applications, like Products 1 and 2.
   - Folders allow delegation of administration rights
     - for example, each head of a department can be granted full ownership of all GCP resources that belong to their department
     - Similarly, access to resources can be limited by folder, so users in one department can only access and create GCP resources within that folder.
 - Resource Manager Roles
   - policies are inherited from top to bottom
   - The organization node also has a Viewer role that grants view access to all resources within an organization.
   - The folder node has multiple roles that mimic the organizational roles but are applied to resources within a folder
     - There is an admin role that provides full control over folders
     - a creator role to browse the hierarchy and create folders
     - a viewer role to view folders and projects below a resource.
   - Similarly, for projects, there is a creator role that allows a user to create new projects, making that user automatically the owner.
   - There is also a project deleter role that grants deletion privileges for projects.
 
_ **Roles** _

 - There are three types of roles in Cloud IAM: basic roles, predefined roles, and custom roles.
 - IAM basic roles apply accross all Google Cloud services in a project
   - In other words, IAM basic roles offer fixed, coarse-grained levels of access.
   - The basic roles are the Owner, Editor, and Viewer roles. 
     - The owner has full administrative access. This includes the ability to add and remove members and delete projects.
     - The editor role has modify and delete access. This allows a developer to deploy applications and modify or configure its resources. 
     - The viewer role has read-only access.
   - All of these roles are concentric, the Owner role includes the permissions of the Editor role, tand he Editor role includes the permissions of the Viewer role
   - There is also a billing administrator role to manage billing and add or remove administrators without the right to change the resources in the project.
   - Each project can have multiple owners, editors, viewers, and billing administrators.
 - IAM predefined roles offer more fine-grained permissions on particular services
  - EG, a group of users is granted the InstanceAdmin role on project_a.
    - This provides the users of that group with all the Compute Engine permissions
    - Grouping these permissions into a role makes them easier to manage
    - The permissions themselves are classes and methods in the APIs.
  - EG, compute.instances.start can be broken down into the service, resource, and verb that mean 
    - that this permission is used to start a stopped Compute Engine instance
  - These permissions usually align with the action’s corresponding REST API.
  - Compute Engine has several predefined IAM roles. Let’s look at three of those:
    - The Compute Admin role provides full control of all Compute Engine resources. 
      - This includes all permissions that start with compute, which means that every action for any type of Compute Engine resource is permitted
    - The Network Admin role contains permissions to create, modify, and delete networking resources, except for firewall rules and SSL certificates
      - In other words, the network admin role allows read-only access to firewall rules, SSL certificates, and instances to view their ephemeral IP addresses. 
    - The Storage Admin role contains permissions to create, modify, and delete disks, images, and snapshots
  - EG,  if your company has someone who manages project images and you don't want them to have the editor role on the project, 
    - grant their account the Storage Admin role on the project
 - IAM custom roles let you define a precise set of permissions
   - A lot of companies use the “least-privilege” model
     - in which each person in your organization is given the minimal amount of privilege needed to do their job. 
   - Let’s say you want to define an “Instance Operator” role to allow some users to start and stop Compute Engine virtual machines, but not reconfigure them.
     - Custom roles allow you to do that

_ **Members** _

 -  the “who” part of “who can do what on which resource.”
 - There are five different types of members: Google Accounts, Service Accounts, Google Groups, Google Workspace domains, and Cloud Identity domains.
   - A Google account represents a developer, an administrator, or any other person who interacts with Google Cloud.
     - Any email address that is associated with a Google account can be an identity, including gmail.com or other domains
     - New users can sign up for a Google account by going to the Google account signup page, without receiving mail through Gmail.
   - A service account is an account that belongs to your application instead of to an individual end user
     - When you run code that is hosted on Google Cloud, you specify the account that the code should run as
     - You can create as many service accounts as needed to represent the different logical components of your application
   - A Google Group is a named collection of Google accounts and service accounts
     - Every group has a unique email address that is associated with the group.
     - Google groups are a convenient way to apply an access policy to a collection of users.
     - You can grant and change access controls for a whole group at once 
     - instead of granting or changing access controls one-at-a-time for individual users or service accounts.
   - A Workspace domain represents a virtual group of all the Google accounts that have been created in an organization's Workspace account
     - Workspace domains represent your organization's internet domain name, such as example.com
     - and when you add a user to your Workspace domain, a new Google account is created for the user inside this virtual group, such as username@example.com
   - Google Cloud customers who are not Workspace customers can get these same capabilities through Cloud Identity
     - Cloud Identity lets you manage users and groups using the Google Admin Console, 
     - but you do not pay for or receive Workspace’s collaboration products such as Gmail, Docs, Drive, and Calendar.
     - Cloud Identity is available in free and premium editions.
     - The premium edition adds capabilities for mobile device management.
   - Now it’s important to note that you cannot use Cloud IAM to create or manage your users or groups
     - Instead, you can use Cloud Identity or Workspace to create and manage users
 - What if I already have a corporate directory, How can you get your users and groups into GCP
   - Using Google Cloud Directory Sync, your administrators can log in and manage GCP resources using the same usernames and passwords they already use
   - This tool synchronizes users and groups from your existing Active Directory or LDAP system with the users and groups in your Cloud Identity domain
   - The synchronization is one-way only; which means that no information in your Active Directory or LDAP map is modified
   - Google Cloud Directory Sync is designed to run scheduled synchronizations without supervision, after its synchronization rules are set up.
 - Single Sign-On (SSO)
   - If you have your identity system, you can continue using your own system and processes with SSO configured
   - When user authentication is required, Google will redirect to your system.
   - If the user is authenticated in your system, access to Google Cloud Platform is given; otherwise, the user is prompted to sign in. 
   - This allows you to also revoke access to GCP
   - If your existing authentication system supports SAML2, SSO configuration is as simple as 3 links and a certificate
   -  Otherwise, you can use a third-party solution, like ADFS, Ping, or Okta.
   
_ **Service Accounts** _

 - another type of member is a service account.
 - A service account is an account that belongs to your application instead of to an individual end user.
 - This provides an identity for carrying out server-to-server interactions in a project without supplying user credentials
   - For example, if you write an application that interacts with Google Cloud Storage, 
     - it must first authenticate to either the Google Cloud Storage XML API or JSON API.
 - You can enable service accounts and grant read-write access to the account on the instance where you plan to run your application
   - Then, program the application to obtain credentials from the service account. 
     - Your application authenticates seamlessly to the API without embedding any secret keys or credentials in your instance, image, or application code.
 - Service accounts are identified by an email address
 - There are three types of service accounts: user-created or custom, built-in, and Google APIs service accounts.
   - By default, all projects come with the built-in Compute Engine default service account
   - Apart from the default service account, all projects come with a Google Cloud Platform APIs service account, 
     - identifiable by the email: project-number@cloudservices.gserviceaccount.com.
   - This is a service account designed specifically to run internal Google processes on your behalf, 
     - and it is automatically granted the Editor role on the project. 
   - Alternatively, you can also start an instance with a custom service account
     - Custom service accounts provide more flexibility than the default service account, but they require more management from you.
     - You can create as many custom service accounts as you need, assign any arbitrary access scopes or Cloud IAM roles to them, 
       - and assign the service accounts to any virtual machine instance.
   - The Default Compute Engine Account
     - this account is automatically created per project. 
     - This account is identifiable by the email project-number-compute@developer.gserviceaccount.com, 
     - and it is automatically granted the Editor role on the project.
     - When you start a new instance using gcloud, the default service account is enabled on that instance
     - You can override this behavior by specifying another service account or by disabling service accounts for the instance. 
 - Scopes are used to determine whether an authenticated identity is authorized.
   - authorization is the process of determining what permissions an authenticated identity has on a set of specified resources.
   - EG, Applications A and B contain Authenticated Identities (or service accounts).
     - Let’s assume that both applications want to use a Cloud Storage bucket. 
     - They each request access from the Google Authorization server, and in return they receive an access token
     - Application A receives an access token with read-only scope, so it can only read from the Cloud Storage bucket.
     - Application B, in contrast, receives an access token with read-write scope, so it can read and modify data in the Cloud Storage bucket.
   - Customizing Scopes for a VM
     - Scopes can be customized when you create an instance using the default service account
     - These scopes can be changed after an instance is created by stopping it.
     - Access scopes are actually the legacy method of specifying permissions for your VM.
     - Before the existence of IAM roles, access scopes were the only mechanism for granting permissions to service accounts.
     - For user-created service accounts, use Cloud IAM roles instead to specify permissions.
 - Service Account Permissions
   - Now, roles for service accounts can also be assigned to groups or users.
   - EG, you create a service account that has the InstanceAdmin role, which has permissions to create, modify, and delete virtual machine instances and disks
   - Then you treat this service account as the resource, and decide who can use it by providing users or a group with the Service Account User role
   - This allows those users to act as that service account to create, modify, and delete virtual machine instances and disks.
   - Users who are Service Account Users for a service account can access all the resources that the service account has access to
   - Therefore, be cautious when granting the Service Account User role to a user or group.
 - Example, Service accounts and Cloud IAM
   - The VMs running component_1 are granted Editor access to project_b using Service Account 1.
   - VMs running component_2 are granted objectViewer access to bucket_1 using an isolated Service Account 2
   - This way you can scope permissions for VMs without re-creating VMs.
   - Essentially, Cloud IAM lets you slice a project into different microservices, 
     - each with access to different resources, by creating service accounts to represent each one.
   - You assign the service accounts to the VMs when they are created, 
     - and you don’t have to ensure that credentials are being managed correctly because GCP manages security for you.
   - How are Service accounts authenticated
 - Service Accounts authenticate with keys
   - Although users require a username and password to authenticate, service accounts use keys. 
   - There are two types of service account keys: GCP-managed keys and user-managed keys.
     - GCP-managed keys are used by GCP services such as App Engine and Compute Engin
       - These keys cannot be downloaded and are automatically rotated and used for a maximum of two weeks
     - User-managed keys are created, downloadable, and managed by users. 
       - When you create a new key pair, you download the private key, which is not retained by Google. 
       - With user-managed keys, you are responsible for security of the private key and other management operations such as key rotation

_ **Cloud IAM best practices** _

 - First, leverage and understand the resource hierarchy. 
   - Specifically, use projects to group resources that share the same trust boundary.
   - Check the policy granted on each resource and make sure you recognize the inheritance. 
   - Because of inheritance, use the "principle of least privilege" when granting roles
   - Finally, audit policies using Cloud audit logs and audit memberships of groups used in policies
 - Grant roles to groups instead of individuals
   - This allows you to update group membership, instead of changing a Cloud IAM policy.
   - If you do this, make sure to audit membership of groups used in policies and control the ownership of the Google group used in Cloud IAM policies.
   - You can also use multiple groups to get better control
   - EG A Network Admin group
     - Some of those members also need a read_write role to a Cloud Storage bucket, but others need the read_only role
     - Adding and removing individuals from all three groups controls their total access
     - Therefore, groups are not only associated with job roles but can exist for the purpose of role assignment.
 - Service Accounts
   - As mentioned before, be very careful when granting the Service Account Users role, 
     - because it provides access to all the resources that the service account has access to.
   - Also, when you create a service account, give it a display name that clearly identifies its purpose, ideally using an established naming convention.
   - As for keys, establish key rotation policies and methods and audit keys with the serviceAccount.keys.list() method.
 - Cloud Identity-Aware Proxy (Cloud IAP)
   - Cloud IAP lets you establish a central authorization layer for applications accessed by HTTPS, 
     - so you can use an application-level access control model instead of relying on network-level firewalls.
   - Applications and resources protected by Cloud IAP can only be accessed through the proxy by users and groups with the correct Cloud IAM role
   - When you grant a user access to an application or resource by Cloud IAP, 
     - they’re subject to the fine-grained access controls implemented by the product in use without requiring a VPN.
   - Cloud IAP performs authentication and authorization checks when a user tries to access a Cloud IAP-secured resource, as shown on the right.
   
**Storage and Database Services**

 - From an application-centered perspective, the technology stores and retrieves the data.
 - Google offers several data storage services to choose from. Cloud Storage, Cloud SQL, Cloud Spanner, Cloud Firestore, and Cloud Bigtable
   - Cloud Storage
     - Object Storage
     - Good for: Binary or object data
     - Such As: Images, media serving, backups
   - Cloud SQL
     - Relational
     - Good for: Web frameworks
     - Such as: CMS, eCommerce
   - Cloud Spanner
     - Relational
     - Good for: RDBMS+scale, HA, HTAP
     - Such as: User metadata, Ad/Fin/MarTech
   - Cloud Firestore
     - Non-Relational
     - Good for: Hierarchical, mobile, web
     - Such as: User profiles, game state
   - Cloud Bigtable
     - Non-Relational
     - Good for: Heavy read + write, events, 
     - Such as: AdTech, financial, IoT
   - BigQuery
     - Good for: Enterprise data warehouse
     - Such as: Analytics, dashboards
     - sits on the edge between data storage and data processing
     - You can store data in BigQuery, but the intended use for BigQuery is big data analysis and interactive querying
 - Storage and Database decision
   - First, ask yourself: Is your data structured? If it’s not, choose Cloud Storage
   - If your data is structured, does your workload focus on analytics? 
     - If it does, you will want to choose Cloud Bigtable or BigQuery, depending on your latency and update needs
   - Otherwise, check whether your data is relational. If it’s not relational, choose Cloud Firestore. 
   - If it is relational, you will want to choose Cloud SQL or Cloud Spanner, depending on your need for horizontal scalability.

_ **Cloud Storage** _

 - Cloud Storage is an object storage service 
 -  it allows world-wide storage and retrieval of any amount of data at any time
 - You can use Cloud Storage for a range of scenarios including 
   - serving website content, storing data for archival and disaster recovery, or distributing large data objects to users via direct download. 
 - Cloud Storage has a couple of key features:
   - It’s scalable to exabytes of data
   - The time to first byte is in milliseconds
   - It has very high availability across all storage classes
   - And It has a single API across those storage classes
 - Some like to think of Cloud Storage as files in a file system but it’s not really a file system
   -  Instead, Cloud Storage is a collection of buckets that you place objects into.
   - You can create directories, but really a directory is just another object that points to different objects in the bucket
   - You’re not going to easily be able to index all of these files like you would in a file system
   - You just have a specific URL to access objects
 - Cloud Storage has four storage classes: Standard, Nearline, Coldline and Archive and each of those storage classes provide 3 location types
   - There’s a multi-region is a large geographic area, such as the United States, that contains two or more geographic places.
   - Dual-region is a specific pair of regions, such as Finland and the Netherlands
   - A region is a specific geographic place, such as London
 - Objects stored in a multi-region or dual-region are geo-redundant. Now, let’s go over each of the storage classes
   - Standard Storage is best for data that is frequently accessed (think of "hot" data) and/or stored for only brief periods of time
     - This is the most expensive storage class but it has no minimum storage duration and no retrieval cost.
     - When used in a region, Standard Storage is appropriate for storing data in the same location as 
       - Google Kubernetes Engine clusters or Compute Engine instances that use the data
       - Co-locating your resources maximizes the performance for data-intensive computations and can reduce network charges.
     - When used in a dual-region, you still get optimized performance when accessing Google Cloud products 
       - that are located in one of the associated regions, 
       - but you also get improved availability that comes from storing data in geographically separate locations.
     - When used in multi-region, Standard Storage is appropriate for storing data that 
       - is accessed around the world, such as serving website content, streaming videos, executing interactive workloads, 
       - or serving data supporting mobile and gaming applications
   - Nearline Storage is a low-cost, highly durable storage service 
     - for storing infrequently accessed data like data backup, long-tail multimedia content, and data archiving
     - Nearline Storage is a better choice than Standard Storage in scenarios where slightly lower availability
     - a 30-day minimum storage duration, and costs for data access are acceptable trade-offs for lowered at-rest storage costs
   - Coldline Storage is a very-low-cost, highly durable storage service for storing infrequently accessed data
     - Coldline Storage is a better choice than Standard Storage or Nearline Storage in scenarios where 
       - slightly lower availability, a 90-day minimum storage duration, and higher costs for data access are acceptable trade-offs 
       - for lowered at-rest storage costs.
   - Archive Storage is the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery
     - Unlike the "coldest" storage services offered by other Cloud providers, your data is available within milliseconds, not hours or days.
     - Unlike other Cloud Storage classes, Archive Storage has no availability SLA, 
       - though the typical availability is comparable to Nearline Storage and Coldline Storage
     - Archive Storage also has higher costs for data access and operations, as well as a 365-day minimum storage duration
     - Archive Storage is the best choice for data that you plan to access less than once a year.
 - Cloud Storage Overview
   - First of all, there are buckets which are required to have a globally unique name and cannot be nested
   - The data that you put into those buckets are objects that inherit the storage class of the bucket 
     - and those objects could be text files, doc files, video files, etc
     - There is no minimum size to those objects and you can scale this as much as you want as long as your quota allows it
   - To access the data, you can use the gsutil command, or either the JSON or XML APIs
 - Changing default storage classes
   - When you upload an object to a bucket, the object is assigned the bucket's storage class, unless you specify a storage class for the object
   - You can change the default storage class of a bucket but you can't change the location type from regional to multi-region/dual-region or vice versa
   - You can also change the storage class of an object that already exists in your bucket 
     - without moving the object to a different bucket or changing the URL to the object.
   - Setting a per-object storage class is useful, for example, if you have objects in your bucket that you want to keep, 
     - but that you don't expect to access frequently
   - In this case, you can minimize costs by changing the storage class of those specific objects to Nearline, Coldline or Archive Storage
   - In order to help manage the classes of objects in your bucket, Cloud Storage offers Object Lifecycle Management
   - Default class is applied to new objects.
   - Regional bucket can never be changed to multi-region/dual-region.
   - Multi-regional bucket can never be changed to regional.
   - Objects can be moved from bucket to bucket.
   - Object Lifecycle Management can manage the classes of objects.
 - Access Control
   - We can use IAM for the project to control which individual user or service account can see the bucket, 
     - list the objects in the bucket, view the names of the objects in the bucket, or create new buckets. 
     - For most purposes, Cloud IAM is sufficient, and roles are inherited from project to bucket to object. 
   - Access control lists or ACLs offer finer control.
   - For even more detailed control, signed URLs provide a cryptographic key that gives time-limited access to a bucket or object. 
   - Finally, a signed policy document further refines the control by determining what kind of file can be uploaded by someone with a signed URL
   - An ACL is a mechanism you can use to define who has access to your buckets and objects, as well as what level of access they have.
     - The maximum number of ACL entries you can create for a bucket or object is 100.
   - Each ACL consists of one or more entries, and these entries consist of two pieces of information
     - A scope, which defines who can perform the specified actions (for example, a specific user or group of users). 
     - And a permission, which defines what actions can be performed (for example, read or write).
 - Signed URLs
   - For some applications, it is easier and more efficient to grant limited-time access tokens that can be used by any user, 
     - instead of using account-based authentication for controlling resource access. 
     - (For example, when you don’t want to require users to have Google accounts). 
   - Signed URLs allow you to do this for Cloud Storage
     - You create a URL that grants read or write access to a specific Cloud Storage resource and specifies when the access expires
     - That URL is signed using a private key associated with a service account. 
     - When the request is received, Cloud Storage can verify that the access-granting URL was issued on behalf of a trusted security principal, 
       - in this case the service account, and delegates its trust of that account to the holder of the URL.
   - After you give out the signed URL, it is out of your control.
     - So you want the signed URL to expire after some reasonable amount of time.
 - Cloud Storage Features
   - Customer-supplied encryption keys when attaching persistent disks to virtual machines
     -  This allows you to supply your own encryption keys instead of the Google-managed keys, which is also available for Cloud Storage.
   - Cloud Storage also provides Object Lifecycle Management which lets you automatically delete or archive objects
   - Another feature is object versioning which allows you to maintain multiple versions of objects in your bucket.
     - You are charged for the versions as if they were multiple files, which is something to keep in mind
   - Cloud Storage also offers directory synchronization so that you can sync a VM directory with a bucket.
   - Object change notifications can be configured for Cloud Storage using Pub/Sub
 - Object Versioning supports the retrieval of objects that are deleted or overwritten
   - In Cloud Storage, objects are immutable, which means that an uploaded object cannot change throughout its storage lifetime
   - To support the retrieval of objects that are deleted or overwritten, Cloud Storage offers the Object Versioning feature
   - Object Versioning can be enabled for a bucket. 
   - Once enabled, Cloud Storage creates an archived version of an object each time the live version of the object is overwritten or deleted.
   - The archived version retains the name of the object but is uniquely identified by a generation number as illustrated on this slide by g1.
   - When Object Versioning is enabled, you can list archived versions of an object, 
     - restore the live version of an object to an older state, or permanently delete an archived version, as needed
   - You can turn versioning on or off for a bucket at any time
   - Turning versioning off leaves existing object versions in place and causes the bucket to stop accumulating new archived object versions.
 - Object Lifecycle Management policies specify actions to be performed on objects that meet certain rules
   - To support common use cases like setting a 
     - Time to Live for objects, archiving older versions of objects, 
     - or "downgrading" storage classes of objects to help manage costs, Cloud Storage offers Object Lifecycle Management. 
   - You can assign a lifecycle management configuration to a bucket
   - The configuration is a set of rules that apply to all the objects in the bucket.
   - So when an object meets the criteria of one of the rules, Cloud Storage automatically performs a specified action on the object.
   - Examples
     - First, downgrade the storage class of objects older than a year to Coldline Storage.
     - Second, delete objects created before a specific date. For example, January 1, 2017.
     - And third, keep only the 3 most recent versions of each object in a bucket with versioning enabled.
   - Object inspection occurs in asynchronous batches, so rules may not be applied immediately
   - Also, updates to your lifecycle configuration may take up to 24 hours to go into effect.
   - This means that when you change your lifecycle configuration, 
     - Object Lifecycle Management may still perform actions based on the old configuration for up to 24 hours
 - Pub/Sub notifications for Cloud Storage
   - You can configure object change notifications for Cloud Storage using Cloud Pub/Sub.
   - Pub/Sub notifications sends information about changes to objects in your buckets to Pub/Sub, 
     - where the information is added to a Pub/Sub topic of your choice in the form of messages.
   - For example, you can track objects that are created and deleted in your bucket
   - Each notification contains information describing both the event that triggered it and the object that changed.
   - You can send notifications to any Pub/Sub topic in any project for which you have sufficient permissions
   - Once received by the Pub/Sub topic, the resulting message can be sent to any number of subscribers to the topic.
   - Pub/Sub notifications are the recommended way to track changes to objects in your Cloud Storage buckets 
     - because they're faster, more flexible, easier to set up, and more cost-effective. 
   - Pub/Sub is Google’s distributed real-time messaging service, which is covered in the Developing Applications track.
 - Data Import Services
   - The Cloud Console allows you to upload individual files to your bucket. 
   - But what if you have to upload terabytes or even petabytes of data? 
   - There are three services that address this: 
     - Transfer Appliance
     - Storage Transfer Service
     - Offline Media Import
   - Transfer Appliance is a hardware appliance you can use to securely migrate large volumes of data 
     - (from hundreds of terabytes up to 1 petabyte) to Google Cloud without disrupting business operations.
   - The Storage Transfer Service enables high-performance imports of online data
     - That data source can be another Cloud Storage bucket, an Amazon S3 bucket, or an HTTP/HTTPS location.
   - Offline Media Import is a third party service where physical media 
     - (such as storage arrays, hard disk drives, tapes, and USB flash drives) 
     - is sent to a provider who uploads the data.
 - Cloud Storage provides strong global consistency
   - When you upload an object to Cloud Storage and you receive a success response, 
     - the object is immediately available for download and metadata operations from any location where Google offers service
   - This is true whether you create a new object or overwrite an existing object.
   - Because uploads are strongly consistent, you will never receive a 404 Not Found response 
     - or stale data for a read-after-write or read-after-metadata-update operation
   - Strong global consistency also extends to deletion operations on objects
     - If a deletion request succeeds, an immediate attempt to download the object or its metadata will result in a 404 Not Found status code
     - You get the 404 error because the object no longer exists after the delete operation succeeds.
   - Bucket listing is strongly consistent. 
     - For example, if you create a bucket, then immediately perform a list buckets operation, the new bucket appears in the returned list of buckets.
   - Object Listing is also strongly consistent
     - For example, if you upload an object to a bucket and then immediately perform a list objects operation, 
     - the new object appears in the returned list of objects.
 - Choosing a Storage Class
   - If you will read your data less than once a year, you should consider using Archive storage. 
   - If you will read your data less than once per 90 days, you should consider using Coldline storage.
   - If you will read your data less than once per 30 days, you should consider using Nearline storage.
   - And if you will be doing reads and writes more often than that, you should consider using Standard storage.
   - You also want to take into account the location type:
     - Use a region to help optimize latency and network bandwidth for data consumers, such as analytics pipelines, that are grouped in the same region
     - Use a dual-region when you want similar performance advantages as regions, but also want the higher availability that comes with being geo-redundant.
     - Use a multi-region when you want to serve content to data consumers that 
       - are outside of the Google network and distributed across large geographic areas, 
       - or when you want the higher data availability that comes with being geo-redundant.

_ **Cloud SQL** _

 - The question really is, should you build your own database solution or use a managed service?
 - Cloud SQL is a fully managed service of either MySQL, PostgreSQL, or Microsoft SQL Server databases.
   - Patches and updates automatically applied
   - You administer MySQL users
   - Cloud SQL supports many clients
     - gcloud sql
     - App Engine, Google Workspace scripts
     - Applications and tools
       - SQL Workbench, Toad
       - External applications using standard MySQL drivers
   - This means that patches and updates are automatically applied but you still have to administer MySQL users with the native authentication tools that come with these databases.
   - Cloud SQL supports many clients, such as Cloud Shell, App Engine and Google Workspace scripts
     - It also supports other applications and tools that you might be used to like SQL Workbench, 
       - Toad and other external applications using standard MySQL drivers.
   - Cloud SQL delivers high performance and scalability with up to 30 TB of storage capacity, 40,000 IOPS, and 416 GB of RAM per instance
     - You can easily scale up to 64 processor cores and scale out with read replicas.
   - Currently, you can use Cloud SQL with either MySQL 5.6, 5.7, or 8.0, PostgreSQL 9.6, 10, 11, or 12, or either of the Web, Express, Standard or Enterprise SQL Server 2017 editions 
   - There is replica service that can replicate data between multiple zones
   - Cloud SQL also provides automated and on-demand backups with point-in-time recovery.
   - You can import and export databases using mysqldump, or import and export CSV files. 
   - Cloud SQL can also scale up, which does require a machine restart or scale out using read replicas. 
     - That being said, if you are concerned about horizontal scalability, you’ll want to consider Cloud Spanner which we’ll cover later in this module.
   - Choosing a connection type to your Cloud SQL instance will influence how secure, performant, and automated it will be
     - If your are connecting an application that is hosted within the same Google Cloud project as your Cloud SQL instance, 
       - and it is collocated in the same region, 
       - choosing the Private IP connection will provide you with the most performant and secure connection using private connectivity.
       - In other words, traffic is never exposed to the public internet.
     - If the application is hosted in another region or project, or if you are trying to connect to your Cloud SQL instance from outside of Google Cloud, you have 3 options
       - In this case, I recommend using the Cloud SQL Proxy, which handles authentication, encryption, and key rotation for you.
       - If you need manual control over the SSL connection, you can generate and periodically rotate the certificates yourself
       - Otherwise, you can use an unencrypted connection by authorizing a specific IP address to connect to your SQL server over its external IP address
 - If you need more than 30 TB of storage space or over 4000 concurrent connections to your database, 
   - or if you want your application design to be responsible for scaling, availability, and location management when scaling up globally, then consider using Cloud Spanner

_ **Cloud Spanner** _
 
 - If Cloud SQL does not fit your requirements because you need horizontal scalability, consider using Cloud Spanner.
 - Cloud Spanner combines the benefits of relational database structure with non-relational horizontal scale
 - Cloud Spanner is a service built for the cloud specifically to combine the benefits of relational database structure with non-relational horizontal scale. 
 - This service can provide petabytes of capacity and offers transactional consistency at global scale, schemas, SQL, and automatic, synchronous replication for high availability.
   - Use cases include financial applications and inventory applications traditionally served by relational database technology.
 - Depending on whether you create a multi-regional or regional instance, you’ll have different monthly uptime SLAs
 - Let’s compare Cloud Spanner with both relational and non-relational databases
   - Like a relational database, Cloud Spanner has schema, SQL, and strong consistency
   - Also, like a non-relational database, Cloud Spanner offers high availability, horizontal scalability, and configurable replication.
   - Cloud Spanner offers the best of the relational and non-relational worlds
   - These features allow for mission-critical uses cases, such as building consistent systems for transactions and inventory management in the financial services and retail industries
 - Cloud Spanner Architecture
   - A Cloud Spanner instance replicates data in N cloud zones, which can be within one region or across several regions
   - The database placement is configurable, meaning you can choose which region to put your database in
   - This architecture allows for high availability and global placement.
 - Data replication is synchronized across zones using Google’s global fiber network
   - The replication of data will be synchronized across zones using Google’s global fiber network.
   - Using atomic clocks ensures atomicity whenever you are updating your data.
 - Chooisng Cloud Spanner
   - If you have outgrown any relational database, are sharding your databases for throughput high performance, need 
     - transactional consistency, global data and strong consistency, 
     - or just want to consolidate your database, 
     - consider using Cloud Spanner.
   - If you don’t need any of these, nor full relational capabilities, consider a NoSQL service such as Cloud Firestore

_ **Cloud Firestore** _

- Cloud Firestore is a NoSQL document database
- Cloud Firestore is a fast, fully managed, serverless, cloud-native NoSQL document database 
  - that simplifies storing, syncing, and querying data for your mobile, web, and IoT apps at global scale
  - Its client libraries provide live synchronization and offline support, and its security features and integrations with Firebase and GCP accelerate building truly serverless apps
- Cloud Firestore also supports ACID transactions, so if any of the operations in the transaction fail and cannot be retried, the whole transaction will fail.
- Also, with automatic multi-region replication and strong consistency, your data is safe and available, even when disasters strike
  - Cloud Firestore even allows you to run sophisticated queries against your NoSQL data without any degradation in performance.
  - This gives you more flexibility in the way you structure your data
- Cloud Firestore is actually the next generation of Cloud Datastore 
  - Cloud Firestore can operate in Datastore mode, making it backwards- compatible with Cloud Datastore
  - By creating a Cloud Firestore database in Datastore mode, you can access Cloud Firestore's improved storage layer while keeping Cloud Datastore system behavior
  - This removes the following Cloud Datastore limitations:
    - Queries are no longer eventually consistent; instead, they are all strongly consistent.
    - Transactions are no longer limited to 25 entity groups.
    - Writes to an entity group are no longer limited to 1 per second
  - Cloud Firestore in Native mode introduces new features such as
    - A new, strongly consistent storage layer
    - A collection and document data model
    - Real-time updates
    - Mobile and Web client libraries
  - Cloud Firestore is backward compatible with Cloud Datastore, but the new data model, real-time updates, and mobile and web client library features are not
    - To access all of the new Cloud Firestore features, you must use Cloud Firestore in Native mode.
    - A general guideline is to use Cloud Firestore in Datastore mode for new server projects, and Native mode for new mobile and web apps.
  - As the next generation of Cloud Datastore, Cloud Firestore is compatible with all Cloud Datastore APIs and client libraries
    -  Existing Cloud Datastore users will be live-upgraded to Cloud Firestore automatically at a future date
- Choosing Cloud FireStore
  - If your schema might change and you need an adaptable database, you need to scale to zero, or you want low maintenance overhead scaling up to terabytes, 
    - consider using Cloud Firestore.
  - Also, if you don’t require transactional consistency, you might want to consider Cloud Bigtable, depending on the cost or size

_ **Cloud BigTable** _

- If you don’t require transactional consistency, you might want to consider Cloud Bigtable.
- Cloud Bigtable is a fully managed NoSQL database with petabyte-scale and very low latency
  - It seamlessly scales for throughput and it learns to adjust to specific access patterns
  - Cloud Bigtable is actually the same database that powers many of Google’s core services, including Search, Analytics, Maps, and Gmail.
- Cloud Bigtable is a great choice for both operational and analytical applications, including IoT, user analytics, and financial data analysis, 
  - because it supports high read and write throughput at low latency
  - It’s also a great storage engine for machine learning applications.
- Cloud Bigtable integrates easily with popular big data tools like Hadoop, Cloud Dataflow, and Cloud Dataproc
  - Plus, Cloud Bigtable supports the open source industry standard HBase API, which makes it easy for your development teams to get started. 
    - Cloud Dataflow and Cloud Dataproc are covered late in the course series
- Cloud BigTable Storage model
  - Cloud Bigtable stores data in massively scalable tables, each of which is a sorted key/value map
  - The table is composed of rows, each of which typically describes a single entity, and columns, which contain individual values for each row
  - Each row is indexed by a single row key, and columns that are related to one another are typically grouped together into a column family
  - Each column is identified by a combination of the column family and a column qualifier, which is a unique name within the column family.
  - Each row/column intersection can contain multiple cells, or versions, at different timestamps, providing a record of how the stored data has been altered over time
    - Cloud Bigtable tables are sparse; if a cell does not contain any data, it does not take up any space.
  - EG, for a hypothetical social network for United States presidents, where each president can follow posts from other presidents. Let me highlight some things:
    - The table contains one column family, the follows family. This family contains multiple column qualifiers.
    - Column qualifiers are used as data. This design choice takes advantage of the sparseness of Cloud Bigtable tables, 
      - and the fact that new column qualifiers can be added as your data changes
    - The username is used as the row key.
      - Assuming usernames are evenly spread across the alphabet, data access will be reasonably uniform across the entire table. 
- processing, which is done through a front-end server pool and nodes, is handled separately from the storage
- A Cloud Bigtable table is sharded into blocks of contiguous rows, called tablets, to help balance the workload of queries.
- Tablets are similar to HBase regions, for those of you who have used the HBase API
- Tablets are stored on Colossus, which is Google's file system, in SSTable format
  -  An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings.
- Cloud Bigtable learns to adjust to specific access patterns. If a certain Bigtable node is frequently accessing a certain subset of data
  - Cloud Bigtable will update the indexes so that other nodes can distribute that workload evenly
- throughput scales linearly, so for every single node that you do add, you're going to see a linear scale of throughput performance, up to hundreds of nodes. 
- Choosing Cloud Bigtable
  - if you need to store more than 1 TB of structured data, have very high volume of writes, 
  - need read/write latency of less than 10 milliseconds along with strong consistency, 
  - or need a storage service that is compatible with the HBase API, 
  - consider using Cloud Bigtable
  - If you don’t need any of these and are looking for a storage service that scales down well, consider using Cloud Firestore.
  - Speaking of scaling, the smallest Cloud Bigtable cluster you can create has three nodes and can handle 30,000 operations per second
    - Remember that you pay for those nodes while they are operational, whether your application is using them or not.

_ **Cloud Memorystore** _

 - Cloud Memorystore is a fully managed Redis service 
 - Cloud Memorystore for Redis provides a fully managed in-memory data store service built on scalable, secure, and highly available infrastructure managed by Google
 - Applications running on GCP can achieve extreme performance by leveraging the highly scalable, available, secure Redis service 
   - without the burden of managing complex Redis deployments.
   - This allows you to spend more time writing code so that you can focus on building great apps
 - Cloud Memorystore also automates complex tasks like enabling high availability, failover, patching, and monitoring 
 - High availability instances are replicated across two zones and provide a 99.9% availability SLA.
 - You can easily achieve the sub-millisecond latency and throughput your applications need.
 - Start with the lowest tier and smallest size, and then grow your instance effortlessly with minimal impact to application availability.
 - Cloud Memorystore can support instances up to 300 GB and network throughput of 12 Gbps
 - Because Cloud Memorystore for Redis is fully compatible with the Redis protocol, 
   - you can lift and shift your applications from open source Redis to Cloud Memorystore without any code changes by using the import/export feature
 - There is no need to learn new tools because all existing tools and client libraries just work.

**Resource Management**

 - Resource Management. Resources in GCP are billable, so managing them means controlling cost
 - There are several methods in place for controlling access to the resources, and there are quotas that limit consumption
 - In most cases, the default quotas can be raised on request, 
   - but having them in place provides a checkpoint or a chance to make sure that this really is a resource you intend to consume in greater quantity

_ **Resource Manager** _
 
 - Resource Manager lets you hierarchically manage resources, building on Cloud IAM
 - The resource manager lets you hierarchically manage resources by project, folder, and organization.
   - This should sound familiar because we covered it in the Cloud IAM module
     - Policies contain a set of roles and members, and policies are set on resources.
     - These resources inherit policies from their parent
     - Therefore, resource policies are a union of parent and resource.
     - Also, keep in mind that if a parent policy is less restrictive, it overrides the more restrictive resource policy.
 - Although IAM policies are inherited top-to-bottom, billing is accumulated from the bottom up,
 - Resource consumption is measured in quantities, like rate of use or time, number of items, or feature use
 - Because a resource belongs to only one project, a project accumulates the consumption of all its resources. 
 - Each project is associated with one billing account, which means that an organization contains all billing accounts
 - organization node is the root node for all Google Cloud Platform resources
   - EG, Bob, who is in control of the organizational domain through the organization admin role
   -  Bob has delegated privileges and access to the individual projects to Alice by making her a project creator
 - Project accumulates the consumption of all its resources
   - Project accumulates the consumption of all its resources
     - Specifically, projects let you enable billing, manage permissions and credentials, and enable service and APIs
   - To interact with Cloud Platform resources, you must provide the identifying project information for every request. 
   - A project can be identified by:
     - The project name, which is a human-readable way to identify your projects, but it isn't used by any Google APIs.
     - There is also the project number, which is automatically generated by the server and assigned to your project
     - And there is the project ID, which is a unique ID that is generated from your project name
 - Resource hierarchy
   - From a physical organization standpoint, resources are categorized as global, regional, or zonal.
     - Images, snapshots, and networks are global resources;
     - External IP addresses are regional resources
     - and instances and disks are zonal resources
   - However, regardless of the type, each resource is organized into a project.
     - This enables each project to have its own billing and reporting

_ **Quotas** _

 - All resources in GCP are subject to project quotas or limits. These typically fall into one of the three categories shown here:
   - How many resources you can create per project. For example, you can only have 5 VPC networks per project
   - How quickly you can make API requests in a project or rate limits. 
     - For example, by default, you can only make 5 administrative actions per second per project when using the Cloud Spanner API.
   - There also regional quotas. For example, by default, you can only have 24 CPUs per region.
 - As your use of GCP expands over time, your quotas may increase accordingly. 
   - If you expect a notable upcoming increase in usage, you can proactively request quota adjustments from the Quotas page in the GCP Console.
   -  This page will also display your current quotas.
 - Why Project Quotas
   - Project quotas prevent runaway consumption in case of an error or malicious attack
     - For example, imagine you accidentally create 100 instead of 10 Compute Engine instances using the gcloud command line.
   - Quotas also prevent billing spikes or surprises
     - Quotas are related to billing, but we will go through how to set up budgets and alerts later, which will really help you manage billing
   - Finally, quotas force sizing consideration and periodic review
     - EG, do you really need a 96-core instance, or can you go with a smaller and cheaper alternative
 - It is also important to mention that quotas are the maximum amount of resources you can create for that resource type as long as those resources are available
   - Quotas do not guarantee that resources will be available at all times
   - EG, If a region is out of local SSDs, you cannot create local SSDs in that region, even if you still had quota for local SSDs
   
_ **Labels and Names** _

 - Labels are a utility for organizing GCP resources
 - Labels are key-value pairs that you can attach to your resources, like VMs, disks, snapshots and images
 - You can create and manage labels using the GCP console, gcloud, or the Resource Manager API, and each resource can have up to 64 labels
 - EG, you could create a label to define the environment of your virtual machines
   - Then you define the label for each of your instances as either production or test
   - Using this label, you could search and list all your production resources for inventory purposes
 - Labels can also be used in scripts to help analyze costs or to run bulk operations on multiple resources.
 - Use labels for
   - I recommend adding labels based on team or cost center to distinguish instances owned by different teams
     - You can use this type of label for cost accounting or budgeting. For example, team:marketing and team:research
   - You can also use labels to distinguish components. For example, component:redis, component:frontend
   - Again, you can label based on environment or stage
   - You should also consider using labels to define an owner or a primary contact for a resource. For example, owner:gaurav, contact:opm
   - Or add labels to your resources to define their state. For example, state:inuse, state:readyfordeletion
 -  it’s important to not confuse labels with tags.
   - Labels, we just learned, are user-defined strings in key-value format that are used to organize resources, and they can propagate through billing. 
   - Tags, on the other hand, are user-defined strings that are applied to instances only and are mainly used for networking, such as applying firewall rules

_ **Billing** _
 
 - Budget and Email Alerts
   - To help with project planning and controlling costs, you can set a budget. Setting a budget lets you track how your spend is growing toward that amount
     - First, you set a budget name and specify which project this budget applies to
     - Then, you can set the budget at a specific amount or match it to the previous month's spend
     - After you determine your budget amount, you can set the budget alerts
       - These alerts send emails to billing admins after spend exceeds a percent of the budget or a specified amount. 
   - In our case, it would send an email when spending reaches 50%, 90%, and 100% of the budget amount.
   - You can even choose to send an alert when the spend is forecasted to exceed the percent of the budget amount by the end of the budget period.
   - In addition to receiving an email, you can use Cloud Pub/Sub notifications to programmatically receive spend updates about this budget.
     - You could even create a Cloud Function that listens to the Pub/Sub topic to automate cost management
 - Labels can help you optimize GCP spend
   - Another way to help optimize your GCP spend is to use labels
   - EG, you could label VM instances that are spread across different regions
     - Maybe these instances are sending most of their traffic to a different continent, which could incur higher costs
     - In that case, you might consider relocating some of those instances or using a caching service like Cloud CDN to cache content closer to your users, 
       - which reduces your networking spend.
   - I recommend labeling all your resources and exporting your billing data to BigQuery to analyze your spend
 - Visualize GCP spend with Data Studio
   - Data Studio turns your data into informative dashboards and reports that are easy to read, easy to share, and fully customizable
   - For example, you can slice and dice your billing reports using your labels. 



**Resource Monitoring**

_ **Google Cloud's Operations Suite** _

 - Google Cloud’s operations suite dynamically discovers cloud resources and application services based on deep integration with Google Cloud and Amazon Web Services
 - Because of its smart defaults, you can have core visibility into your cloud platform in minutes.
 - This provides you with access to powerful data and analytics tools plus collaboration with many different third-party software providers
 - Google Cloud’s operations suite has services for monitoring, logging, error reporting, fault tracing, and debugging
 - You only pay for what you use, and there are free usage allotments so that you can get started with no upfront fees or commitments
 - Now, in most other environments, these services are handled by completely different packages, or by a loosely integrated collection of software
 - When you see these functions working together in a single, comprehensive, and integrated service, 
   - you'll realize how important that is to creating reliable, stable, and maintainable applications. 
 - Google Cloud’s operations suite also supports a rich and growing ecosystem of technology partners, as shown on this slide.
 - This helps expand the IT ops, security, and compliance capabilities available to Google Cloud customers

_ **Monitoring** _

 - Monitoring is important to Google because it is at the base of site reliability engineering, or SRE.
 - SRE is a discipline that applies aspects of software engineering to operations whose goals are to create ultra-scalable and highly reliable software systems
 - This discipline has enabled Google to build, deploy, monitor, and maintain some of the largest software systems in the world
 - Cloud Monitoring dynamically configures monitoring after resources are deployed 
   - and has intelligent defaults that allow you to easily create charts for basic monitoring activities
 - This allows you to monitor your platform, system, and application metrics by ingesting data, such as metrics, events, and metadata
   - You can then generate insights from this data through dashboards, charts, and alerts. 
   - For example, you can configure and measure uptime and health checks that send alerts via email.
 - A Workspace is the root entity that holds monitoring and configuration information in Cloud Monitoring
   - Each Workspace can have between 1 and 100 monitored projects, including one or more Google Cloud projects and any number of AWS accounts.
   - You can have as many Workspaces as you want, but Google Cloud projects and AWS accounts can't be monitored by more than one Workspace.
   - A Workspace contains the custom dashboards, alerting policies, uptime checks, notification channels, and group definitions that you use with your monitored projects
   - A Workspace can access metric data from its monitored projects, but the metric data and log entries remain in the individual projects.
   - The first monitored Google Cloud project in a Workspace is called the hosting project, and it must be specified when you create the Workspace
   - The name of that project becomes the name of your Workspace
   - To access an AWS account, you must configure a project in Google Cloud to hold the AWS Connector
   - Because Workspaces can monitor all your Google Cloud projects in a single place, a Workspace is a “single pane of glass” 
     - through which you can view resources from multiple Google Cloud projects and AWS accounts
   -  All users of Google Cloud’s operations suite with access to that Workspace have access to all data by default.
   - This means that a role assigned to one person on one project applies equally to all projects monitored by that Workspace.
   - In order to give people different roles per-project and to control visibility to data, consider placing the monitoring of those projects in separate Workspaces.
 - Cloud Monitoring allows you to create custom dashboards that contain charts of the metrics that you want to monitor
   - For example, you can create charts that display your instances’ CPU utilization, the packets or bytes sent and received by those instances,
     - and the packets or bytes dropped by the firewall of those instances.
   - In other words, charts provide visibility into the utilization and network traffic of your VM instances, as shown on this slide
   - These charts can be customized with filters to remove noise, groups to reduce the number of time series, and aggregates to group multiple time series together
 - create alerting policies that notify you when specific conditions are met
   - EG, you can create an alerting policy when the network egress of your VM instance goes above a certain threshold for a specific timeframe.
   - When this condition is met, you or someone else can be automatically notified through email, SMS, or other channels in order to troubleshoot this issue
   - You can also create an alerting policy that monitors your usage of Google Cloud’s operations suite and alerts you when you approach the threshold for billing
   - Best practices
     - alerting on symptoms, and not necessarily causes. For example, you want to monitor failing queries of a database and then identify whether the database is down
     - make sure that you are using multiple notification channels, like email and SMS. This helps avoid a single point of failure in your alerting strategy
     - customizing your alerts to the audience’s need by describing what actions need to be taken or what resources need to be examined
     - avoid noise, because this will cause alerts to be dismissed over time.  
       - Specifically, adjust monitoring alerts so that they are actionable and don’t just set up alerts on everything possible
 - Uptime checks can be configured to test the availability of your public services from locations around the world,
   - The type of uptime check can be set to HTTP, HTTPS, or TCP
   - The resource to be checked can be an App Engine application, a Compute Engine instance, a URL of a host, or an AWS instance or load balancer. 
   - For each uptime check, you can create an alerting policy and view the latency of each global location
 - Cloud Monitoring can access some metrics without the Monitoring agent,
   - including CPU utilization, some disk traffic metrics, network traffic, and uptime information.
   - However, to access additional system resources and application services, you should install the Monitoring agent
   - The Monitoring agent is supported for Compute Engine and EC2 instances.
 - If the standard metrics provided by Cloud Monitoring do not fit your needs, you can create custom metrics
   - EG, imagine a game server that has a capacity of 50 users. 
   - What metric indicator might you use to trigger scaling events? 
   - From an infrastructure perspective, you might consider using CPU load or perhaps network traffic load as values that are somewhat correlated with the number of users.
   - But with a Custom Metric, you could actually pass the current number of users directly from your application into Cloud Monitoring

_ **Logging** _

 - Cloud Logging allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud and AWS
 - It is a fully managed service that performs at scale and can ingest application and system log data from thousands of VMs
 - Logging includes storage for logs, a user interface called the Logs Viewer, and an API to manage logs programmatically
 - The service lets you read and write log entries, search and filter your logs, and create log-based metrics
 - Logs are only retained for 30 days, but you can export your logs to Cloud Storage buckets, BigQuery datasets, and Pub/Sub topics. 
 - Exporting logs to Cloud Storage makes sense for storing logs for more than 30 days, but why should you export to BigQuery or Pub/Sub
 - Exporting logs to BigQuery allows you to analyze logs and even visualize them in Data Studio. 
   - BigQuery runs extremely fast SQL queries on gigabytes to petabytes of data.
   - This allows you to analyze logs, such as your network traffic, 
   - so that you can better understand traffic growth to forecast capacity, network usage to optimize network traffic expenses, or network forensics to analyze incidents
 - want to visualize your logs, I recommend connecting your BigQuery tables to Data Studio
   - Data Studio transforms your raw data into the metrics and dimensions that you can use to create easy-to-understand reports and dashboards.
 - you can also export logs to Cloud Pub/Sub. This enables you to stream logs to applications or endpoints.
 - Similar to the Cloud Monitoring agent, it’s a best practice to install the Logging agent on all your VM instances.
   - The Logging agent can be installed with these two simple commands, which you could include in your startup script
   - This agent is supported for Compute Engine and EC2 instances.

_ **Error Reporting** _
 
 - Error Reporting counts, analyzes, and aggregates the errors in your running cloud services
 - A centralized error management interface displays the results with sorting and filtering capabilities, 
   - and you can even set up real-time notifications when new errors are detected.
 - In terms of programming languages, the exception stack trace parser is able to process Go, Java, .NET, Node.js, PHP, Python, and Ruby. 

_ **Tracing** _

 - Cloud Trace is a distributed tracing system that collects latency data from your applications and displays it in the Cloud Console
 - You can track how requests propagate through your application and receive detailed near real-time performance insights.
 - Cloud Trace automatically analyzes all of your application's traces to generate in-depth latency reports 
   - that surface performance degradations and can capture traces from App Engine, HTTP(S) load balancers, and applications instrumented with the Cloud Trace API.
 - Managing the amount of time it takes for your application to handle incoming requests and perform operations is an important part of managing overall application performance
 - Cloud Trace is actually based on the tools used at Google to keep our services running at extreme scale

_ **Debugging** _
 
 - Cloud Debugger is a feature of Google Cloud that lets you inspect the state of a running application, in real time, without stopping or slowing it
 - Specifically, the debugger adds less than 10ms to the request latency when the application state is captured. 
   - In most cases, this is not noticeable by users
 - These features allow you to understand the behavior of your code in production and analyze its state to locate those hard-to-find bugs
 - With just a few mouse clicks, you can take a snapshot of your running application’s state or inject a new logging statement
 - Cloud Debugger supports multiple languages, including Java, Python, Go, Node.js and Ruby.
